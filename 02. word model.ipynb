{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from funcs.logic import process_pdfs_to_faiss_with_positions\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import TypedDict, List, Optional\n",
    "from langgraph.graph import StateGraph\n",
    "from funcs.logic import get_qa_score\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document as DocxDocument\n",
    "from tqdm import tqdm  # ÏßÑÌñâ ÏÉÅÌô© ÌëúÏãúÏö©\n",
    "\n",
    "source_dir = \"path/to/sources\"\n",
    "output_dir = \"data/case2\"\n",
    "# embedding_model = \"intfloat/multilingual-e5-small\"\n",
    "embedding_model = \"intfloat/multilingual-e5-large-instruct\"\n",
    "chunk_size = 500  # Í∞Å Ï≤≠ÌÅ¨Ïùò ÌÅ¨Í∏∞ (Î¨∏Ïûê Ïàò)\n",
    "chunk_overlap = 50  # Ï≤≠ÌÅ¨ Í∞Ñ Í≤πÏπ® ÌÅ¨Í∏∞ (Î¨∏Ïûê Ïàò)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1Ô∏è‚É£ Word Î¨∏ÏÑúÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂úÌïòÍ≥† Ï†ÑÏ≤òÎ¶¨ÌïòÎäî Ìï®Ïàò\n",
    "def extract_and_preprocess_text_from_docx(file_path):\n",
    "    try:\n",
    "        doc = DocxDocument(file_path)\n",
    "        # Î¨∏Îã® ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú (Îπà Ï§Ñ Ï†úÏô∏)\n",
    "        paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "\n",
    "        # Î¨∏Îã®ÏùÑ Í∞úÌñâÎ¨∏ÏûêÎ°ú Í≤∞Ìï©\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "\n",
    "        # ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨\n",
    "        text = preprocess_text(text)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {file_path} ÌååÏùº Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 2Ô∏è‚É£ ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
    "def preprocess_text(text):\n",
    "    # ÌïúÍ∏Ä Ïú†ÎãàÏΩîÎìú Ï†ïÍ∑úÌôî (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "    # Î∂àÌïÑÏöîÌïú ÌäπÏàò Î¨∏Ïûê Î∞è Í≥µÎ∞± Ï†úÍ±∞\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\u2060\\ufeff]', '', text)  # Î≥¥Ïù¥ÏßÄ ÏïäÎäî ÌäπÏàò Î¨∏Ïûê Ï†úÍ±∞\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Ïó∞ÏÜçÎêú Í≥µÎ∞± Ï†úÍ±∞\n",
    "\n",
    "    # Í∞Å Ï§Ñ ÏïûÎí§ Í≥µÎ∞± Ï†úÍ±∞\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "\n",
    "    # Îπà Ï§Ñ Ï†úÍ±∞ (Ïó∞ÏÜçÎêú Ï§ÑÎ∞îÍøà)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# 3Ô∏è‚É£ ÌÖçÏä§Ìä∏Î•º Ï≤≠ÌÅ¨Î°ú Î∂ÑÌï†ÌïòÎäî Ìï®Ïàò\n",
    "def create_chunks_from_texts(texts, metadata_list, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"],\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    for text, metadata in zip(texts, metadata_list):\n",
    "        # ÌÖçÏä§Ìä∏Í∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎßå Ï≤≠ÌÇπ ÏßÑÌñâ\n",
    "        if text:\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            # Í∞Å Ï≤≠ÌÅ¨Î≥ÑÎ°ú Î¨∏ÏÑú ÏÉùÏÑ±\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_metadata = metadata.copy()  # ÏõêÎ≥∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î≥µÏÇ¨\n",
    "                chunk_metadata[\"chunk_id\"] = i    # Ï≤≠ÌÅ¨ ID Ï∂îÍ∞Ä\n",
    "                documents.append(Document(page_content=chunk, metadata=chunk_metadata))\n",
    "\n",
    "    return documents\n",
    "\n",
    "# 4Ô∏è‚É£ Î©îÏù∏ Ìï®Ïàò: Ïó¨Îü¨ Í∞úÏùò Word ÌååÏùºÏùÑ ÏùΩÏñ¥ÏôÄÏÑú Î≤°ÌÑ∞ DB ÏÉùÏÑ±\n",
    "def process_word_documents_to_vector_db(doc_dir, output_dir,\n",
    "                                        embedding_model_name=\"intfloat/multilingual-e5-small\",\n",
    "                                        chunk_size=500, chunk_overlap=50):\n",
    "    # Word ÌååÏùº Î™©Î°ù ÏàòÏßë\n",
    "    word_files = [f for f in os.listdir(doc_dir) if f.endswith(\".docx\")]\n",
    "\n",
    "    if not word_files:\n",
    "        print(\"‚ö†Ô∏è Ï≤òÎ¶¨Ìï† Word ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üîç Ï¥ù {len(word_files)}Í∞úÏùò Word ÌååÏùºÏùÑ Ï≤òÎ¶¨Ìï©ÎãàÎã§...\")\n",
    "\n",
    "    # ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "    texts = []\n",
    "    metadata_list = []\n",
    "\n",
    "    for file_name in tqdm(word_files, desc=\"ÌååÏùº Ï≤òÎ¶¨ Ï§ë\"):\n",
    "        file_path = os.path.join(doc_dir, file_name)\n",
    "        text = extract_and_preprocess_text_from_docx(file_path)\n",
    "\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (ÌååÏùºÎ™Ö, Í≤ΩÎ°ú Îì±)\n",
    "            metadata = {\n",
    "                \"source\": file_name,\n",
    "                \"full_path\": file_path,\n",
    "                \"file_type\": \"docx\"\n",
    "            }\n",
    "            metadata_list.append(metadata)\n",
    "\n",
    "    # ÌÖçÏä§Ìä∏ Ï≤≠ÌÇπ\n",
    "    print(f\"üìÑ ÌÖçÏä§Ìä∏Î•º Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞ {chunk_size}(Í≤πÏπ® {chunk_overlap})Î°ú Î∂ÑÌï†Ìï©ÎãàÎã§...\")\n",
    "    documents = create_chunks_from_texts(texts, metadata_list, chunk_size, chunk_overlap)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"‚ö†Ô∏è Ï≤òÎ¶¨Ìï† Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚úÖ {len(documents)}Í∞úÏùò Ï≤≠ÌÅ¨Í∞Ä ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.\")\n",
    "\n",
    "    # ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú\n",
    "    print(f\"üß† ÏûÑÎ≤†Îî© Î™®Îç∏ '{embedding_model_name}'ÏùÑ Î°úÎìúÌï©ÎãàÎã§...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    # FAISS Î≤°ÌÑ∞ DB ÏÉùÏÑ±\n",
    "    print(\"üî¢ Î≤°ÌÑ∞ DBÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§...\")\n",
    "    vector_db = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    # Î≤°ÌÑ∞ DB Ï†ÄÏû•\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    vector_db.save_local(output_dir)\n",
    "    print(f\"‚úÖ '{output_dir}'Ïóê Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "\n",
    "    return vector_db"
   ],
   "id": "fb0a37957c526307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1Ô∏è‚É£ FAISS Î≤°ÌÑ∞ DB Î°úÎìú\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"Dongjin-kr/ko-reranker\")\n",
    "qa_model = AutoModelForSequenceClassification.from_pretrained(\"Dongjin-kr/ko-reranker\")\n",
    "\n",
    "\n",
    "# Î©îÏù∏ Ìï®Ïàò Ïã§Ìñâ\n",
    "vector_db = process_word_documents_to_vector_db(\n",
    "    source_dir,\n",
    "    output_dir,\n",
    "    embedding_model,\n",
    "    chunk_size,\n",
    "    chunk_overlap\n",
    ")\n"
   ],
   "id": "a1a3f5fea5ab0af9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LangGraph pipeline Ïã§Ìñâ\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_docs: Optional[List]\n",
    "    reranked_docs: Optional[List]\n",
    "    top_docs: Optional[List]\n",
    "    answer: Optional[str]\n",
    "\n",
    "\n",
    "def retrieve_documents(state: QAState):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_db.similarity_search_with_score(question, k=30)\n",
    "    return {\"retrieved_docs\": retrieved_docs}\n",
    "\n",
    "\n",
    "def rerank_documents(state: QAState):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "\n",
    "    scored_docs = []\n",
    "    _score = []\n",
    "    for doc, _ in retrieved_docs:\n",
    "        score = get_qa_score(question, doc.page_content, qa_tokenizer, qa_model)\n",
    "        scored_docs.append((doc, score))\n",
    "        _score.append(score)\n",
    "\n",
    "    _average = sum(_score) / len(_score)\n",
    "\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    # top_docs = [doc for doc, _ in reranked_docs[:10]]\n",
    "    top_docs = []\n",
    "    for doc, score in reranked_docs[:10]:\n",
    "        doc.metadata[\"score\"] = score\n",
    "        if score > _average:\n",
    "            top_docs.append(doc)\n",
    "\n",
    "    return {\n",
    "        \"reranked_docs\": reranked_docs,\n",
    "        \"top_docs\": top_docs,\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1ebe538767e16ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph = StateGraph(QAState)\n",
    "\n",
    "graph.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph.add_node(\"rerank_documents\", rerank_documents)\n",
    "\n",
    "graph.set_entry_point(\"retrieve_documents\")\n",
    "graph.add_edge(\"retrieve_documents\", \"rerank_documents\")\n",
    "\n",
    "\n",
    "graph.set_finish_point(\"retrieve_documents\")\n",
    "\n",
    "qa_graph = graph.compile()\n"
   ],
   "id": "cb14b616314723d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# CSV ÌååÏùºÏùÑ ÏùΩÏñ¥ ÎîïÏÖîÎÑàÎ¶¨ Î¶¨Ïä§Ìä∏Î°ú Ï†ÄÏû•\n",
    "data_list = []\n",
    "with open(\"path/to/sources\", newline='', encoding=\"utf-8-sig\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)  # Í∞Å ÌñâÏùÑ ÎîïÏÖîÎÑàÎ¶¨Î°ú Î≥ÄÌôò\n",
    "    for row in reader:\n",
    "        op = {\n",
    "            'code':row['code'],\n",
    "            'item':row['item'],\n",
    "            'exp' : row['exp'],\n",
    "              }\n",
    "        result = qa_graph.invoke({\"question\": f\"{row['item']}\"})\n",
    "\n",
    "        op['result'] = result\n",
    "        data_list.append(op)"
   ],
   "id": "24dcdd07ff09b248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "result_list = []\n",
    "for _documents in data_list:\n",
    "    for _docs in _documents['result']['top_docs']:\n",
    "        _dummy = {\n",
    "            \"code\":_documents['code'],\n",
    "            \"item\":_documents['item'],\n",
    "            \"page_content\": _docs.page_content,\n",
    "            \"score\": _docs.metadata['score'],\n",
    "        }\n",
    "        result_list.append(_dummy)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(result_list)\n",
    "\n",
    "\n",
    "\n",
    "mean_scores = df.groupby('code')['score'].mean().reset_index()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Î∞ïÏä§ ÌîåÎ°Ø Í∑∏Î¶¨Í∏∞\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='code', y='score', data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Score Distribution by Code')\n",
    "plt.show()"
   ],
   "id": "d0865223501f811d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_list",
   "id": "7eed4c1cd87ac692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "f5414d39012c6d3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_list",
   "id": "192ac20df8ff0177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c059ce86e15154cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
