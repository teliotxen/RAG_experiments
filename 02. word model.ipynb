{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from funcs.logic import process_pdfs_to_faiss_with_positions\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import TypedDict, List, Optional\n",
    "from langgraph.graph import StateGraph\n",
    "from funcs.logic import get_qa_score\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from docx import Document as DocxDocument\n",
    "from tqdm import tqdm  # 진행 상황 표시용\n",
    "\n",
    "source_dir = \"path/to/sources\"\n",
    "output_dir = \"data/case2\"\n",
    "# embedding_model = \"intfloat/multilingual-e5-small\"\n",
    "embedding_model = \"intfloat/multilingual-e5-large-instruct\"\n",
    "chunk_size = 500  # 각 청크의 크기 (문자 수)\n",
    "chunk_overlap = 50  # 청크 간 겹침 크기 (문자 수)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1️⃣ Word 문서에서 텍스트 추출하고 전처리하는 함수\n",
    "def extract_and_preprocess_text_from_docx(file_path):\n",
    "    try:\n",
    "        doc = DocxDocument(file_path)\n",
    "        # 문단 텍스트 추출 (빈 줄 제외)\n",
    "        paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]\n",
    "\n",
    "        # 문단을 개행문자로 결합\n",
    "        text = \"\\n\".join(paragraphs)\n",
    "\n",
    "        # 텍스트 전처리\n",
    "        text = preprocess_text(text)\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {file_path} 파일 처리 중 오류 발생: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 2️⃣ 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    # 한글 유니코드 정규화 (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "\n",
    "    # 불필요한 특수 문자 및 공백 제거\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\u2060\\ufeff]', '', text)  # 보이지 않는 특수 문자 제거\n",
    "    text = re.sub(r'\\s+', ' ', text)  # 연속된 공백 제거\n",
    "\n",
    "    # 각 줄 앞뒤 공백 제거\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    text = '\\n'.join(lines)\n",
    "\n",
    "    # 빈 줄 제거 (연속된 줄바꿈)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# 3️⃣ 텍스트를 청크로 분할하는 함수\n",
    "def create_chunks_from_texts(texts, metadata_list, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \" \", \"\"],\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    for text, metadata in zip(texts, metadata_list):\n",
    "        # 텍스트가 있는 경우에만 청킹 진행\n",
    "        if text:\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            # 각 청크별로 문서 생성\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_metadata = metadata.copy()  # 원본 메타데이터 복사\n",
    "                chunk_metadata[\"chunk_id\"] = i    # 청크 ID 추가\n",
    "                documents.append(Document(page_content=chunk, metadata=chunk_metadata))\n",
    "\n",
    "    return documents\n",
    "\n",
    "# 4️⃣ 메인 함수: 여러 개의 Word 파일을 읽어와서 벡터 DB 생성\n",
    "def process_word_documents_to_vector_db(doc_dir, output_dir,\n",
    "                                        embedding_model_name=\"intfloat/multilingual-e5-small\",\n",
    "                                        chunk_size=500, chunk_overlap=50):\n",
    "    # Word 파일 목록 수집\n",
    "    word_files = [f for f in os.listdir(doc_dir) if f.endswith(\".docx\")]\n",
    "\n",
    "    if not word_files:\n",
    "        print(\"⚠️ 처리할 Word 파일이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔍 총 {len(word_files)}개의 Word 파일을 처리합니다...\")\n",
    "\n",
    "    # 텍스트 추출 및 전처리\n",
    "    texts = []\n",
    "    metadata_list = []\n",
    "\n",
    "    for file_name in tqdm(word_files, desc=\"파일 처리 중\"):\n",
    "        file_path = os.path.join(doc_dir, file_name)\n",
    "        text = extract_and_preprocess_text_from_docx(file_path)\n",
    "\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "            # 메타데이터 생성 (파일명, 경로 등)\n",
    "            metadata = {\n",
    "                \"source\": file_name,\n",
    "                \"full_path\": file_path,\n",
    "                \"file_type\": \"docx\"\n",
    "            }\n",
    "            metadata_list.append(metadata)\n",
    "\n",
    "    # 텍스트 청킹\n",
    "    print(f\"📄 텍스트를 청크 크기 {chunk_size}(겹침 {chunk_overlap})로 분할합니다...\")\n",
    "    documents = create_chunks_from_texts(texts, metadata_list, chunk_size, chunk_overlap)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"⚠️ 처리할 문서가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\"✅ {len(documents)}개의 청크가 생성되었습니다.\")\n",
    "\n",
    "    # 임베딩 모델 로드\n",
    "    print(f\"🧠 임베딩 모델 '{embedding_model_name}'을 로드합니다...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    # FAISS 벡터 DB 생성\n",
    "    print(\"🔢 벡터 DB를 생성합니다...\")\n",
    "    vector_db = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    # 벡터 DB 저장\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    vector_db.save_local(output_dir)\n",
    "    print(f\"✅ '{output_dir}'에 벡터 데이터베이스 저장 완료\")\n",
    "\n",
    "    return vector_db"
   ],
   "id": "fb0a37957c526307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1️⃣ FAISS 벡터 DB 로드\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"Dongjin-kr/ko-reranker\")\n",
    "qa_model = AutoModelForSequenceClassification.from_pretrained(\"Dongjin-kr/ko-reranker\")\n",
    "\n",
    "\n",
    "# 메인 함수 실행\n",
    "vector_db = process_word_documents_to_vector_db(\n",
    "    source_dir,\n",
    "    output_dir,\n",
    "    embedding_model,\n",
    "    chunk_size,\n",
    "    chunk_overlap\n",
    ")\n"
   ],
   "id": "a1a3f5fea5ab0af9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LangGraph pipeline 실행\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    retrieved_docs: Optional[List]\n",
    "    reranked_docs: Optional[List]\n",
    "    top_docs: Optional[List]\n",
    "    answer: Optional[str]\n",
    "\n",
    "\n",
    "def retrieve_documents(state: QAState):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_db.similarity_search_with_score(question, k=30)\n",
    "    return {\"retrieved_docs\": retrieved_docs}\n",
    "\n",
    "\n",
    "def rerank_documents(state: QAState):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "\n",
    "    scored_docs = []\n",
    "    _score = []\n",
    "    for doc, _ in retrieved_docs:\n",
    "        score = get_qa_score(question, doc.page_content, qa_tokenizer, qa_model)\n",
    "        scored_docs.append((doc, score))\n",
    "        _score.append(score)\n",
    "\n",
    "    _average = sum(_score) / len(_score)\n",
    "\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    # top_docs = [doc for doc, _ in reranked_docs[:10]]\n",
    "    top_docs = []\n",
    "    for doc, score in reranked_docs[:10]:\n",
    "        doc.metadata[\"score\"] = score\n",
    "        if score > _average:\n",
    "            top_docs.append(doc)\n",
    "\n",
    "    return {\n",
    "        \"reranked_docs\": reranked_docs,\n",
    "        \"top_docs\": top_docs,\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1ebe538767e16ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph = StateGraph(QAState)\n",
    "\n",
    "graph.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph.add_node(\"rerank_documents\", rerank_documents)\n",
    "\n",
    "graph.set_entry_point(\"retrieve_documents\")\n",
    "graph.add_edge(\"retrieve_documents\", \"rerank_documents\")\n",
    "\n",
    "\n",
    "graph.set_finish_point(\"retrieve_documents\")\n",
    "\n",
    "qa_graph = graph.compile()\n"
   ],
   "id": "cb14b616314723d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# CSV 파일을 읽어 딕셔너리 리스트로 저장\n",
    "data_list = []\n",
    "with open(\"path/to/sources\", newline='', encoding=\"utf-8-sig\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)  # 각 행을 딕셔너리로 변환\n",
    "    for row in reader:\n",
    "        op = {\n",
    "            'code':row['code'],\n",
    "            'item':row['item'],\n",
    "            'exp' : row['exp'],\n",
    "              }\n",
    "        result = qa_graph.invoke({\"question\": f\"{row['item']}\"})\n",
    "\n",
    "        op['result'] = result\n",
    "        data_list.append(op)"
   ],
   "id": "24dcdd07ff09b248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "result_list = []\n",
    "for _documents in data_list:\n",
    "    for _docs in _documents['result']['top_docs']:\n",
    "        _dummy = {\n",
    "            \"code\":_documents['code'],\n",
    "            \"item\":_documents['item'],\n",
    "            \"page_content\": _docs.page_content,\n",
    "            \"score\": _docs.metadata['score'],\n",
    "        }\n",
    "        result_list.append(_dummy)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(result_list)\n",
    "\n",
    "\n",
    "\n",
    "mean_scores = df.groupby('code')['score'].mean().reset_index()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 박스 플롯 그리기\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='code', y='score', data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Score Distribution by Code')\n",
    "plt.show()"
   ],
   "id": "d0865223501f811d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result_list",
   "id": "7eed4c1cd87ac692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "f5414d39012c6d3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_list",
   "id": "192ac20df8ff0177",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c059ce86e15154cb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
