import os
import numpy as np
import re
import unicodedata
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
import fitz
import uuid
import torch
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.docstore.document import Document
from tqdm import tqdm
from torch import sigmoid
from collections import defaultdict


def load_documents_from_folder(folder_path):
    documents = []
    sources = []
    headers = []

    for filename in os.listdir(folder_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

                # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                lines = content.split('\n')

                current_header = None
                for line in lines:
                    line = line.strip()
                    if not line:  # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
                        continue

                    # #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í—¤ë” í…ìŠ¤íŠ¸ í™•ì¸
                    if line.startswith('-----## '):
                        current_header = line
                        continue

                    # ë¬¸ì„œ ì •ë³´ ì €ì¥
                    documents.append(line)
                    sources.append(filename)
                    headers.append(current_header)
    return documents, sources, headers


# 6ï¸âƒ£ ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def search_documents(embedding_model, query, index, headers, sources, documents,  top_k=3):
    query_vector = np.array([embedding_model.encode(query)])

    # Faissë¥¼ ì‚¬ìš©í•´ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    distances, indices = index.search(query_vector, top_k)

    results = []
    for i, idx in enumerate(indices[0]):
        result = {
            "íŒŒì¼ëª…": sources[idx],
            "í—¤ë”": headers[idx] if headers[idx] else "ì—†ìŒ",
            "ë‚´ìš©": documents[idx],
            "ìœ ì‚¬ë„ ì ìˆ˜": 1 / (1 + distances[0][i])  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (0~1 ì‚¬ì´)
        }
        results.append(result)

    return results

def spliter(_txt, name):
    _title = ""
    _source = []
    for line in _txt.split("\n"):
        if "-----##" in line:
            _title = line.split("-----##")[1]
        else:
            _source.append({"text": line, "source": f"{name}-{_title}"})
    return _source


def extract_text_from_pdf(pdf_path):
    # ì‚¬ìš©ì ì§€ì • CropBox ì—¬ë°± (ë‹¨ìœ„: pt)
    CROPBOX_MARGIN = {
        "top": 8.05,      # 2cm
        "bottom": 56.7,   # 2cm
        "left": 56.7,     # 1.5cm + 0.5cm(ì œë³¸ ì—¬ë°±)
        "right": 42.525   # 1.5cm
    }
    """PDF íŒŒì¼ì—ì„œ CropBoxë¥¼ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            texts = []
            metadata_list = []
            for page_num, page in enumerate(pdf.pages, start=1):
                # í˜ì´ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
                x0, y0, x1, y1 = page.bbox

                # CropBox ì ìš©í•˜ì—¬ í¬ë¡­ëœ ì˜ì—­ ì„¤ì •
                cropped_bbox = (
                    x0 + CROPBOX_MARGIN["left"],
                    y0 + CROPBOX_MARGIN["bottom"],
                    x1 - CROPBOX_MARGIN["right"],
                    y1 - CROPBOX_MARGIN["top"]
                )

                # CropBox ì ìš©
                cropped_page = page.within_bbox(cropped_bbox)
                text = cropped_page.extract_text() or ""
                text = preprocess_text(text)

                if text:
                    metadata = {
                        "source": os.path.basename(pdf_path),
                        "full_path": pdf_path,
                        "file_type": "pdf",
                        "page_number": page_num  # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                    }
                    texts.append(text)
                    metadata_list.append(metadata)
        return texts, metadata_list
    except Exception as e:
        print(f"âš ï¸ {pdf_path} ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return [], []


def preprocess_text(text):
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
    text = unicodedata.normalize('NFC', text)
    # ë³´ì´ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ ë¬¸ì ì œê±° (ì¢Œ-to-ìš° ë§ˆí¬ ë“±)
    text = re.sub(r'[â€‹â€Œâ€â ï»¿]', '', text)
    # í•œê¸€ ë’¤ì— ì˜¤ëŠ” % ê¸°í˜¸ ì œê±°
    text = re.sub(r'([\uac00-\ud7a3])%', r'\1', text)
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def clean_percent_symbols(text):
    # í•œê¸€ ë¬¸ì ë’¤ì— ë¶™ì€ '%' ì œê±°
    cleaned_text = re.sub(r'([\uac00-\ud7a3])%', r'\1', text)
    return cleaned_text


def create_chunks_from_texts(texts, metadata_list, chunk_size=500, chunk_overlap=50):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ë©° PDF ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", "? ", "! ", " ", ""],
        length_function=len
    )
    documents = []
    for text, metadata in zip(texts, metadata_list):
        chunks = text_splitter.split_text(text)
        for i, chunk in enumerate(chunks):
            chunk_metadata = metadata.copy()
            chunk_metadata["chunk_id"] = i  # ì²­í¬ ID ì¶”ê°€
            documents.append(Document(page_content=chunk, metadata=chunk_metadata))
    return documents


def process_documents_to_vector_db(pdf_dir, output_dir,
                                   embedding_model_name="intfloat/multilingual-e5-small",
                                   chunk_size=500, chunk_overlap=50):
    """PDF ë¬¸ì„œë¥¼ ë³€í™˜, ì „ì²˜ë¦¬ í›„ ë²¡í„° DBë¡œ ì €ì¥"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(".pdf")]
    if not pdf_files:
        print("âš ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ” ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    texts = []
    metadata_list = []

    for file_name in tqdm(pdf_files, desc="íŒŒì¼ ë³€í™˜ ì¤‘"):
        file_path = os.path.join(pdf_dir, file_name)
        pdf_texts, pdf_metadata = extract_text_from_pdf(file_path)
        texts.extend(pdf_texts)
        metadata_list.extend(pdf_metadata)

    print(f"ğŸ“„ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ í¬ê¸° {chunk_size}(ê²¹ì¹¨ {chunk_overlap})ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
    documents = create_chunks_from_texts(texts, metadata_list, chunk_size, chunk_overlap)

    if not documents:
        print("âš ï¸ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… {len(documents)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ '{embedding_model_name}'ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)

    print("ğŸ”¢ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    vector_db = FAISS.from_documents(documents, embedding_model)

    vector_db.save_local(output_dir)
    print(f"âœ… '{output_dir}'ì— ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")

    return vector_db
  # PyMuPDF


def extract_text_with_positions(pdf_path):
    doc = fitz.open(pdf_path)
    texts = []
    metadata = []

    for page_num, page in enumerate(doc):
        blocks = page.get_text("dict")["blocks"]
        for block in blocks:
            if "lines" in block:
                for line in block["lines"]:
                    line_text = " ".join([span["text"] for span in line["spans"]])
                    bbox = line["bbox"]
                    if line_text.strip():
                        texts.append(line_text)
                        metadata.append({
                            "source": pdf_path,
                            "page": page_num,
                            "bbox": bbox,
                        })

    return texts, metadata


def create_chunks_with_metadata(texts, metadata, chunk_size, chunk_overlap):
    chunks = []
    for i, text in enumerate(texts):
        start = 0
        while start < len(text):
            end = min(len(text), start + chunk_size)
            chunk_text = text[start:end]
            chunk_meta = metadata[i].copy()
            chunk_meta["offset"] = start
            chunk_meta["length"] = end - start
            chunks.append(Document(page_content=chunk_text, metadata=chunk_meta))
            start += chunk_size - chunk_overlap
    return chunks


def process_pdfs_to_faiss_with_positions(pdf_dir, output_dir,
                                         embedding_model_name="intfloat/multilingual-e5-small",
                                         chunk_size=500, chunk_overlap=50):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(".pdf")]
    if not pdf_files:
        print("âš ï¸ PDF ì—†ìŒ")
        return

    texts = []
    metadata_list = []

    for file in tqdm(pdf_files, desc="PDF ì¶”ì¶œ ì¤‘"):
        file_path = os.path.join(pdf_dir, file)
        file_texts, file_metadata = extract_text_with_positions(file_path)
        texts.extend(file_texts)
        metadata_list.extend(file_metadata)

    documents = create_chunks_with_metadata(texts, metadata_list, chunk_size, chunk_overlap)
    print(documents)
    print(f"âœ… ì´ {len(documents)}ê°œ ì²­í¬ ìƒì„±")

    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
    vector_db = FAISS.from_documents(documents, embedding_model)
    vector_db.save_local(output_dir)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_dir}")
    return vector_db


def highlight_matches_in_pdf(pdf_path, matched_chunks, output_pdf_path):
    doc = fitz.open(pdf_path)
    for chunk in matched_chunks:
        meta = chunk.metadata
        page = doc[meta["page"]]
        bbox = meta.get("bbox")
        if bbox:
            page.add_highlight_annot(bbox)
    doc.save(output_pdf_path)
    print(f"âœ… ê²°ê³¼ ì €ì¥ë¨: {output_pdf_path}")


def export_evidence_pdfs(result, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    # source_path + page_num ê¸°ì¤€ìœ¼ë¡œ bbox ëª¨ìœ¼ê¸°
    grouped_evidence = defaultdict(list)

    for item in result['answer']['input_documents']:
        doc_obj = item
        source_path = doc_obj.metadata['source']
        page_num = int(doc_obj.metadata['page'])
        bbox_data = doc_obj.metadata['bbox']
        grouped_evidence[(source_path, page_num)].append(bbox_data)

    for (source_path, page_num), bboxes in grouped_evidence.items():
        try:
            if not os.path.isfile(source_path):
                print(f"[Warning] íŒŒì¼ ì—†ìŒ: {source_path}")
                continue

            doc = fitz.open(source_path)

            if page_num < 0 or page_num >= len(doc):
                print(f"[Warning] ì˜ëª»ëœ í˜ì´ì§€ ë²ˆí˜¸: {page_num} (ì´ í˜ì´ì§€: {len(doc)})")
                doc.close()
                continue

            page = doc[page_num]

            for bbox_data in bboxes:
                if isinstance(bbox_data, (list, tuple)):
                    bbox = fitz.Rect(*bbox_data)
                elif isinstance(bbox_data, fitz.Rect):
                    bbox = bbox_data
                else:
                    raise ValueError(f"[Error] bbox í˜•ì‹ì´ ì˜ëª»ë¨: {bbox_data}")

                # í˜•ê´‘ í‘œì‹œ
                page.draw_rect(
                    bbox,
                    color=None,
                    fill=(1, 1, 0),
                    fill_opacity=0.5,
                    overlay=True
                )

            # ìƒˆë¡œìš´ PDFì— í•´ë‹¹ í˜ì´ì§€ë§Œ ì‚½ì…
            new_doc = fitz.open()
            new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)

            filename = f"{os.path.splitext(os.path.basename(source_path))[0]}_page{page_num}_{uuid.uuid4().hex[:8]}.pdf"
            save_path = os.path.join(output_dir, filename)
            new_doc.save(save_path)

            print(f"[âœ“] ì €ì¥ ì™„ë£Œ: {save_path}")

            new_doc.close()
            doc.close()

        except Exception as e:
            print(f"[Error] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def get_qa_score(question, context, qa_tokenizer, qa_model):
    inputs = qa_tokenizer(question, context, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = qa_model(**inputs)
        # ko-reranker ëª¨ë¸ì€ logitsì´ [1] í˜•íƒœë¡œ ë‚˜ì˜¤ë¯€ë¡œ squeeze
        # relevance_score = outputs.logits.squeeze().item()
        relevance_score = sigmoid(outputs.logits).squeeze().item()
    return relevance_score


from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
import os
from tqdm import tqdm


def process_pdfs_to_chroma_with_positions(
    pdf_dir,
    output_dir,
    embedding_model_name="intfloat/multilingual-e5-small",
    chunk_size=500,
    chunk_overlap=50
):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(".pdf")]
    if not pdf_files:
        print("âš ï¸ PDF ì—†ìŒ")
        return

    texts = []
    metadata_list = []

    for file in tqdm(pdf_files, desc="ğŸ“„ PDF ì¶”ì¶œ ì¤‘"):
        file_path = os.path.join(pdf_dir, file)
        file_texts, file_metadata = extract_text_with_positions(file_path)
        texts.extend(file_texts)
        metadata_list.extend(file_metadata)

    documents = create_chunks_with_metadata(texts, metadata_list, chunk_size, chunk_overlap)
    print(f"âœ… ì´ {len(documents)}ê°œ ì²­í¬ ìƒì„±")

    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)

    # Chromaì— ì €ì¥
    vector_db = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=output_dir
    )

    vector_db.persist()
    print(f"âœ… Chroma ì €ì¥ ì™„ë£Œ: {output_dir}")

    return vector_db


def export_evidence_pdfs_beta(result, output_dir):
    os.makedirs(output_dir, exist_ok=True)

    # source_path + page_num ê¸°ì¤€ìœ¼ë¡œ bbox ëª¨ìœ¼ê¸°
    grouped_evidence = defaultdict(list)

    for item in result:
        doc_obj = item
        source_path = doc_obj.metadata['source']
        page_num = int(doc_obj.metadata['page'])
        bbox_data = doc_obj.metadata['bbox']
        grouped_evidence[(source_path, page_num)].append(bbox_data)

    for (source_path, page_num), bboxes in grouped_evidence.items():
        try:
            if not os.path.isfile(source_path):
                print(f"[Warning] íŒŒì¼ ì—†ìŒ: {source_path}")
                continue

            doc = fitz.open(source_path)

            if page_num < 0 or page_num >= len(doc):
                print(f"[Warning] ì˜ëª»ëœ í˜ì´ì§€ ë²ˆí˜¸: {page_num} (ì´ í˜ì´ì§€: {len(doc)})")
                doc.close()
                continue

            page = doc[page_num]

            for bbox_data in bboxes:
                if isinstance(bbox_data, (list, tuple)):
                    bbox = fitz.Rect(*bbox_data)
                elif isinstance(bbox_data, fitz.Rect):
                    bbox = bbox_data
                else:
                    raise ValueError(f"[Error] bbox í˜•ì‹ì´ ì˜ëª»ë¨: {bbox_data}")

                # í˜•ê´‘ í‘œì‹œ
                page.draw_rect(
                    bbox,
                    color=None,
                    fill=(1, 1, 0),
                    fill_opacity=0.5,
                    overlay=True
                )

            # ìƒˆë¡œìš´ PDFì— í•´ë‹¹ í˜ì´ì§€ë§Œ ì‚½ì…
            new_doc = fitz.open()
            new_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)

            filename = f"{os.path.splitext(os.path.basename(source_path))[0]}_page{page_num}_{uuid.uuid4().hex[:8]}.pdf"
            save_path = os.path.join(output_dir, filename)
            new_doc.save(save_path)

            # print(f"[âœ“] ì €ì¥ ì™„ë£Œ: {save_path}")

            new_doc.close()
            doc.close()

        except Exception as e:
            print(f"[Error] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
